{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape and download all links to the local file system due to http speed limit\n",
    "import urllib.request\n",
    "html_doc = ''\n",
    "def init():\n",
    "    with urllib.request.urlopen('https://en.wikipedia.org/wiki/List_of_Greek_mythological_figures') as response:\n",
    "        html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open(\"wiki/Greek-index.html\", 'r')\n",
    "html_doc =  f.read()\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "all_links = soup.find_all('a')\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all of the relevant web pages from all the links\n",
    "def download_pages():\n",
    "    counter = 0\n",
    "    for link in all_links:\n",
    "        if(link.has_attr('href') and \"/wiki/\" in link['href']):\n",
    "            if counter/len(all_links) % 10 == 0:\n",
    "                print(str(counter/len(all_links) * 100) + '%')\n",
    "            href = link['href']\n",
    "            if '.jpg' or '.svg' or '.png' in href:\n",
    "                continue\n",
    "            urllib.request.urlretrieve(\"https://en.wikipedia.org\"+href, href[href.rfind('/') + 1:] + '.html')\n",
    "    href = \"/wiki/List_of_Greek_mythological_figures\"\n",
    "    urllib.request.urlretrieve(\"https://en.wikipedia.org\" + href, href[href.rfind('/') + 1:] + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h3.span.text\n",
    "soup.h3.next_sibling\n",
    "sibling = soup.h3\n",
    "counter = 0\n",
    "h3_tag = soup.h3\n",
    "h3_sibling_content = soup.h3.next_sibling.next_sibling\n",
    "\n",
    "#Go back 1 sibling to be able to grab the first h3 tag into the list\n",
    "clean_siblings = [s for s in h3_tag.previous_sibling.next_siblings if (str(s).strip() != \"\" and (s.name == \"h3\" or s.name == \"div\" or s.name == \"table\" or s.name==\"ul\"))]\n",
    "clean_siblings = clean_siblings[:-7]\n",
    "clean_siblings.remove(clean_siblings[5])\n",
    "\n",
    "it = iter(clean_siblings)\n",
    "heading_content_dict= {}\n",
    "for sibling in it:\n",
    "    heading_content_dict[sibling.text.replace(\"[edit]\",\"\").replace(\"/\",\" & \")] = next(it)\n",
    "print(len(heading_content_dict))\n",
    "print(heading_content_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"wiki/Hera.html\",'r')\n",
    "#soup = BeautifulSoup(str(f.read()), 'html.parser')\n",
    "\n",
    "def get_data_info_box(soup):\n",
    "    info_box = soup.findAll(\"table\", { \"class\" : \"infobox\" })\n",
    "    tr_list = []\n",
    "    for e in info_box:\n",
    "        tr_list.extend(e.findAll(\"tr\"))\n",
    "    return tr_list\n",
    "\n",
    "def scrape_info_box(info_box_data_tr, divinity_name):\n",
    "    parents = {}\n",
    "    children = {}\n",
    "    \n",
    "    for tr in info_box_data_tr:\n",
    "        if tr.th and (tr.th.text == \"Children\" or tr.th.text == \"Parents\" or tr.th.text == \"Offspring\"):\n",
    "            print(tr.th.text + \":\")\n",
    "            st = re.sub(r'\\s(or|and)\\s', ',',tr.td.text) #Get rid of 'or' 'and' stop words\n",
    "            st = strip_accents(st)\n",
    "            result = [x.strip().lower() for x in re.split(\";|,|\\n\",re.sub(r'(\\[|\\().*?(\\]|\\))','',st)) if x != '']\n",
    "            if tr.th.text == \"Children\" or tr.th.text == \"Offspring\" or tr.th.text == \"Parents\":\n",
    "                    #master_dict.setdefault(vr[vr.rfind(\"/\") + 1:].lower(), {})\n",
    "                if divinity_name in master_dict:\n",
    "                    master_dict[divinity_name] = dict(master_dict[divinity_name],**{str(tr.th.text).lower(): result})\n",
    "                else:\n",
    "                    master_dict[divinity_name] = {str(tr.th.text).lower(): result}\n",
    "                        \n",
    "                if tr.th.text == \"Children\" or tr.th.text == \"Offspring\":\n",
    "                    children[divinity_name] = result\n",
    "                elif tr.th.text == \"Parents\":\n",
    "                    parents[divinity_name] = result\n",
    "            print(result)\n",
    "    return (parents, children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(uri_fragment, fill_in_missing = False):\n",
    "    f = open(uri_fragment[1:] + \".html\", 'r')\n",
    "    sub_doc = f.read()\n",
    "    soup_major_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "\n",
    "    info_box_tr = get_data_info_box(soup_major_sub)\n",
    "    \n",
    "    uri_fragment_clean = re.sub(r'(\\[|\\().*?(\\]|\\))','', uri_fragment).strip().replace('_','')\n",
    "           \n",
    "    #wiki/Pallas\n",
    "    divinity_name = strip_accents(uri_fragment_clean[uri_fragment_clean.rfind(\"/\") + 1:].lower()) #Pallas\n",
    "    \n",
    "\n",
    "    mappings_uri[uri_fragment] = divinity_name  \n",
    "    mappings_name[divinity_name] = uri_fragment\n",
    "    \n",
    "    pair = scrape_info_box(info_box_tr, divinity_name)\n",
    "    \n",
    "    print()\n",
    "    return pair\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = {}\n",
    "scraped_set = set()\n",
    "mappings_uri = {} #uri to lowercase name\n",
    "mappings_name = {} #lowercase name to uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = heading_content_dict[\"Major gods and goddesses\"]\n",
    "soup_major = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_major.find_all(\"td\")\n",
    "\n",
    "for td in values:\n",
    "    if not td.a.img:\n",
    "        vr = td.a['href']\n",
    "        print(vr)\n",
    "        \n",
    "        tvr = re.sub(r'(\\[|\\().*?(\\]|\\))','', vr).strip().replace('_','')\n",
    "        scraped_set.add(tvr[tvr.rfind(\"/\") + 1:])\n",
    "        \n",
    "        result = scrape_page(vr)\n",
    "        major_parents = result[0]\n",
    "        major_children = result[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_set.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = heading_content_dict[\"Primordial deities\"]\n",
    "soup_prime = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_prime.find_all(\"tr\")\n",
    "\n",
    "prime_parents = {}\n",
    "prime_children = {}\n",
    "\n",
    "for tr in values:\n",
    "    if tr.contents[3].a:\n",
    "        \n",
    "        vr = tr.contents[3].a['href']\n",
    "        print(vr)\n",
    "        tvr = re.sub(r'(\\[|\\().*?(\\]|\\))','', vr).strip().replace('_','')\n",
    "\n",
    "        scraped_set.add(tvr[tvr.rfind(\"/\") + 1:])\n",
    "        \n",
    "        result = scrape_page(vr)\n",
    "        \n",
    "        prime_parents = result[0]\n",
    "        prime_children = result[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #prime_mappings_uri[tr.contents[3].a['href']] = vr[vr.rfind(\"/\") + 1:].lower()\n",
    "        #prime_mappings_name[vr[vr.rfind(\"/\") + 1:].lower()] = tr.contents[3].a['href']\n",
    "\n",
    "def id_missing_genealogy(scraped_set, parents_dict, children_dict):\n",
    "    id_missing_parents = []\n",
    "    id_missing_children = []\n",
    "    for p in scraped_set:\n",
    "        if p not in parents_dict:\n",
    "            #print(\"No Parent(s): \" + p)\n",
    "            id_missing_parents.append(p)\n",
    "        if p not in children_dict:\n",
    "            #print(\"No Children(s):\" + p)\n",
    "            id_missing_children.append(p)\n",
    "    return (id_missing_parents, id_missing_children)\n",
    "\n",
    "id_missing_result = id_missing_genealogy(scraped_set, prime_parents, prime_children )\n",
    "missing_parents = id_missing_result[0]\n",
    "missing_children = id_missing_result[1]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from py2neo import Graph\n",
    "print(mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def fill_in_missing(uri, index, mapping_uri):\n",
    "    parents_mapping = {} #current god to its parents\n",
    "    children_mapping = {} #current god to its children\n",
    "    incomplete_results = {}\n",
    "    \n",
    "    print(\"Uri: \" + uri)\n",
    "    f = open(uri[1:] + '.html','r')\n",
    "    sub_doc = f.read()\n",
    "\n",
    "    soup_t_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "    b = soup_t_sub.findAll(\"p\")\n",
    "        \n",
    "    for i in b:\n",
    "        st = re.sub(r'\\<[^>]*\\>' ,'',str(i))\n",
    "        \n",
    "        st = re.sub(r'(\\[|\\().*?(\\]|\\))','', st)\n",
    "        translator = str.maketrans('', '', \",\")\n",
    "    \n",
    "        st = st.translate(translator)\n",
    "        st = strip_accents(st)\n",
    "        regex = \"\"\n",
    "\n",
    "        #k = re.search(r\"(\\b[A-Z]+\\w*\\b)\\s+(?:\\b[a-z]*\\b\\s+)*(born|progeny|daughter|father|mother|son|offspring)\\s?(?:.*?)\\s?of\\s?(?:.*?)\\s?(\\b[A-Z]+[a-z]+\\b)(?:\\s+and\\s+)?(?:\\b[a-z]*\\b\\s+)*(\\b[A-Z]+[a-z]+\\b)?\", st)\n",
    "        matches = re.finditer(r\"(\\b[A-Z]+\\w*\\b)\\s+(?:\\b[a-z]*\\b\\s+)*(\\bborn|progeny|daughter|father|mother|son|offspring\\b)\\s?(?:.*?)\\s?(?:of|by)\\s?(?:.*?)\\s?(\\b[A-Z]+[a-z]+\\b)(?:\\s+and\\s+)?(?:\\b[a-z]*\\b\\s+)*(\\b[A-Z]+[a-z]+\\b)?\", st)\n",
    "        for k in matches:\n",
    "            divinity_name = mapping_uri[uri].lower()\n",
    "            a = str(k.group(1)).lower()\n",
    "            b = str(k.group(2)).lower()\n",
    "            c = str(k.group(3)).lower()\n",
    "            d = str(k.group(4)).lower()\n",
    "            if a.lower() == divinity_name:\n",
    "                #wiki/Pallas\n",
    "                #pallas daughter of _ and _\n",
    "                if(b in ('daughter','born','son', 'offspring','progeny')):\n",
    "                    parents_mapping.setdefault(a,[]).extend([c, d])\n",
    "                #pallas mother of _ and _\n",
    "                elif(k.group(2) in ('father','mother')):\n",
    "                    children_mapping.setdefault(a,[]).extend([c, d])\n",
    "\n",
    "            elif c.lower() == divinity_name or d.lower() == divinity_name:\n",
    "                #x son of pallas and _\n",
    "                if(k.group(2) in ('daughter','born','son', 'offspring','progeny')):\n",
    "                    children_mapping.setdefault(divinity_name.title(),[]).append(k.group(1))\n",
    "                #y father of pallas and _\n",
    "                elif(k.group(2) in ('father','mother')):\n",
    "                    parents_mapping.setdefault(divinity_name.title(),[]).append(k.group(1))\n",
    "            else:\n",
    "                incomplete_results.setdefault(uri,[]).append([index, k]) \n",
    "                print(\"Incomplete: \" + uri + \" : \" + k.group(0))\n",
    "    #print()\n",
    "    return {\"Parents\": parents_mapping, \"Children\": children_mapping, \"Incomplete\": incomplete_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(master_dict)\n",
    "master_incomplete_results = []\n",
    "def processIncompleteList(missingList, mapping_name, mapping_uri):\n",
    "    results = {}\n",
    "    incomplete_results = [] #Record those with incomplete results\n",
    "    for idx, p in enumerate(missingList):\n",
    "        print(\"Current Index: \" + str(idx))\n",
    "        results = fill_in_missing(mapping_name[p.lower()], idx, mapping_uri)\n",
    "        master_dict.setdefault(p.lower(),{})\n",
    "        if len(results[\"Incomplete\"]) > 0:\n",
    "            incomplete_results.append(results)\n",
    "        if(len(results[\"Parents\"]) > 0):\n",
    "            print(results[\"Parents\"])\n",
    "            master_dict[p.lower()].setdefault(\"parents\",[]).extend(list(results[\"Parents\"].values())[0])\n",
    "        if(len(results[\"Children\"]) > 0):\n",
    "            master_dict[p.lower()].setdefault(\"children\",[]).extend(list(results[\"Children\"].values())[0])\n",
    "        print()\n",
    "    return incomplete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prime processing\n",
    "master_incomplete_results += processIncompleteList(missing_parents, mappings_name, mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict['uranus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Ourea\n",
    "#Tatarus should be father of Giants\n",
    "print(len(master_incomplete_results))\n",
    "for i in master_incomplete_results:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#master_dict['tartarus']['children'] = [\"giants\"]\n",
    "master_dict['achlys']['parents'].append('nyx')\n",
    "#master_incomplete_results.clear() #discard rest of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(master_dict))\n",
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = heading_content_dict[\"Titans and Titanesses\"]\n",
    "soup_titan = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_titan.find_all(\"tr\")\n",
    "\n",
    "values = values[2:]\n",
    "\n",
    "titan_parents = {}\n",
    "titan_children = {}\n",
    "\n",
    "scraped_set.clear()\n",
    "\n",
    "for tr in values:\n",
    "    if len(tr.contents) < 4:\n",
    "        continue\n",
    "    if tr.contents[3].a:\n",
    "          \n",
    "        vr = tr.contents[3].a['href']\n",
    "        print(vr)\n",
    "        tvr = re.sub(r'(\\[|\\().*?(\\]|\\))','', vr).strip().replace('_','')\n",
    "\n",
    "        scraped_set.add(tvr[tvr.rfind(\"/\") + 1:])\n",
    "        \n",
    "        result = scrape_page(vr)\n",
    "        \n",
    "        titan_parents = result[0]\n",
    "        titan_children = result[1]\n",
    "        \n",
    "id_missing_result = id_missing_genealogy(scraped_set, titan_parents, titan_children )\n",
    "missing_parents = id_missing_result[0]\n",
    "missing_children = id_missing_result[1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_incomplete_results += processIncompleteList(missing_parents, mappings_name, mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_incomplete_results += processIncompleteList(missing_children, mappings_name, mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict['prometheus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = heading_content_dict['Gigantes and other \"giants\"']\n",
    "                           \n",
    "soup_titan = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_titan.find_all(\"li\")\n",
    "for li in values:\n",
    "    print(li.find_all(\"a\",href=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heading_content_dict['Gigantes and other \"giants\"']\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
