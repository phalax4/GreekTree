{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape and download all links to the local file system due to http speed limit\n",
    "import urllib.request\n",
    "html_doc = ''\n",
    "def init():\n",
    "    with urllib.request.urlopen('https://en.wikipedia.org/wiki/List_of_Greek_mythological_figures') as response:\n",
    "        html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open(\"wiki/Greek-index.html\", 'r')\n",
    "html_doc =  f.read()\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "all_links = soup.find_all('a')\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all of the relevant web pages from all the links\n",
    "def download_pages():\n",
    "    counter = 0\n",
    "    for link in all_links:\n",
    "        if(link.has_attr('href') and \"/wiki/\" in link['href']):\n",
    "            if counter/len(all_links) % 10 == 0:\n",
    "                print(str(counter/len(all_links) * 100) + '%')\n",
    "            href = link['href']\n",
    "            if '.jpg' or '.svg' or '.png' in href:\n",
    "                continue\n",
    "            urllib.request.urlretrieve(\"https://en.wikipedia.org\"+href, href[href.rfind('/')+1:] + '.html')\n",
    "    href = \"/wiki/List_of_Greek_mythological_figures\"\n",
    "    urllib.request.urlretrieve(\"https://en.wikipedia.org\"+href, href[href.rfind('/')+1:] + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h3.span.text\n",
    "soup.h3.next_sibling\n",
    "sibling = soup.h3\n",
    "counter = 0\n",
    "h3_tag = soup.h3\n",
    "h3_sibling_content = soup.h3.next_sibling.next_sibling\n",
    "\n",
    "#Go back 1 sibling to be able to grab the first h3 tag into the list\n",
    "clean_siblings = [s for s in h3_tag.previous_sibling.next_siblings if (str(s).strip() != \"\" and (s.name == \"h3\" or s.name == \"div\" or s.name == \"table\" or s.name==\"ul\"))]\n",
    "clean_siblings = clean_siblings[:-7]\n",
    "clean_siblings.remove(clean_siblings[5])\n",
    "\n",
    "it = iter(clean_siblings)\n",
    "heading_content_dict= {}\n",
    "for sibling in it:\n",
    "    heading_content_dict[sibling.text.replace(\"[edit]\",\"\").replace(\"/\",\" & \")] = next(it)\n",
    "print(len(heading_content_dict))\n",
    "print(heading_content_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = heading_content_dict[\"Major gods and goddesses\"]\n",
    "soup_major = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_major.find_all(\"td\")\n",
    "\n",
    "major_set = set()\n",
    "\n",
    "major_mappings_uri = {} #uri to lowercase name\n",
    "major_mappings_name = {} #lowercase name to uri\n",
    "major_parents = {}\n",
    "major_children = {}\n",
    "for td in values:\n",
    "    if not td.a.img:\n",
    "        print(td.a['href'])\n",
    "        f = open(td.a['href'][1:] + \".html\", 'r')\n",
    "        sub_doc = f.read()\n",
    "        soup_major_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "        box_info = soup_major_sub.find_all(\"tr\")\n",
    "        \n",
    "        vr = td.a['href']\n",
    "        major_mappings_uri[td.a['href']] = vr[vr.rfind(\"/\") + 1:].lower()\n",
    "        \n",
    "        major_mappings_name[vr[vr.rfind(\"/\") + 1:].lower()] = td.a['href']\n",
    "        major_set.add(vr[vr.rfind(\"/\") + 1:])\n",
    "\n",
    "        for tr in box_info:\n",
    "            if tr.th and (tr.th.text == \"Children\" or tr.th.text == \"Parents\" or tr.th.text == \"Offspring\"):\n",
    "                print(tr.th.text + \":\")\n",
    "                st = re.sub(r'\\s(or|and)\\s', ',',tr.td.text) #Get rid of 'or' 'and' stop words\n",
    "                #st = tr.td.text\n",
    "                result = [x.strip().lower() for x in re.sub(r'(\\[|\\().*?(\\]|\\))','',st).split(\",\") if x != '']\n",
    "                if tr.th.text == \"Children\" or tr.th.text == \"Offspring\" or tr.th.text == \"Parents\":\n",
    "                    #master_dict.setdefault(vr[vr.rfind(\"/\") + 1:].lower(), {})\n",
    "                    if vr[vr.rfind(\"/\") + 1:].lower() in master_dict:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = dict(master_dict[vr[vr.rfind(\"/\") + 1:].lower()],**{str(tr.th.text).lower(): result})\n",
    "                    else:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                        \n",
    "                    if tr.th.text == \"Children\" or tr.th.text == \"Offspring\":\n",
    "                        major_children[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                    elif tr.th.text == \"Parents\":\n",
    "                        major_parents[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                #master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                print(result) #Parse out citation square brackets\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = heading_content_dict[\"Primordial deities\"]\n",
    "soup_prime = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_prime.find_all(\"tr\")\n",
    "\n",
    "#links = []\n",
    "\n",
    "prime_set = set()\n",
    "\n",
    "prime_parents = {}\n",
    "prime_children = {}\n",
    "\n",
    "prime_uris = []\n",
    " \n",
    "prime_mappings_uri = {}  #uri to name\n",
    "prime_mappings_name = {} #name to uri\n",
    "for tr in values:\n",
    "    if tr.contents[3].a:\n",
    "        f = open(tr.contents[3].a['href'][1:] + '.html','r')\n",
    "        prime_uris.append(tr.contents[3].a['href'][1:])\n",
    "        sub_doc = f.read()\n",
    "        soup_prime_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "        box_info = soup_prime_sub.find_all(\"tr\")\n",
    "        print(tr.contents[3].a['href'])\n",
    "        \n",
    "        vr = re.sub(r'(\\[|\\().*?(\\]|\\))','',tr.contents[3].a['href']).strip().replace('_','')\n",
    "        \n",
    "        prime_mappings_uri[tr.contents[3].a['href']] = vr[vr.rfind(\"/\") + 1:].lower()\n",
    "        prime_mappings_name[vr[vr.rfind(\"/\") + 1:].lower()] = tr.contents[3].a['href']\n",
    "        \n",
    "        prime_set.add(vr[vr.rfind(\"/\") + 1:])\n",
    "        for tr in box_info:\n",
    "            if tr.th and (tr.th.text == \"Children\" or tr.th.text == \"Parents\" or tr.th.text == \"Offspring\"):\n",
    "                st = re.sub(r'\\s(or|and)\\s', ',',tr.td.text) #Get rid of 'or' 'and' stop words\n",
    "                #st = tr.td.text\n",
    "                st = st.replace('with', '')\n",
    "                result = [x.strip().lower() for x in re.split(r',|\\n', re.sub(r'(\\[|\\().*?(\\]|\\))','',st)) if x != '']\n",
    "                #v = re.sub(r'(\\[|\\().*?(\\]|\\))','',tr.contents[3].a['href']).replace('_','').strip()\n",
    "               \n",
    "                print(tr.th.text + \":\")\n",
    "                print(result) #Parse out citation square brackets\n",
    "                \n",
    "                if tr.th.text == \"Children\" or tr.th.text == \"Offspring\" or tr.th.text == \"Parents\":\n",
    "                    #master_dict.setdefault()\n",
    "                    if vr[vr.rfind(\"/\") + 1:].lower() in master_dict:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = dict(master_dict[vr[vr.rfind(\"/\") + 1:].lower()],**{str(tr.th.text).lower(): result})\n",
    "                    else:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                        \n",
    "                    if tr.th.text == \"Children\" or tr.th.text == \"Offspring\":\n",
    "                        prime_children[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                    elif tr.th.text == \"Parents\":\n",
    "                        prime_parents[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "\n",
    "        print()\n",
    "\n",
    "missing_parents = []\n",
    "missing_children = []\n",
    "for p in prime_set:\n",
    "    if p not in prime_parents:\n",
    "        #print(\"No Parent(s): \" + p)\n",
    "        missing_parents.append(p)\n",
    "    if p not in prime_children:\n",
    "        #print(\"No Children(s):\" + p)\n",
    "        missing_children.append(p)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)\n",
    "#print(master_dict['aphrodite'])\n",
    "#from py2neo import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def fill_in_missing(uri, index, mapping_uri):\n",
    "    parents_mapping = {} #current god to its parents\n",
    "    children_mapping = {} #current god to its children\n",
    "    incomplete_results = {}\n",
    "    \n",
    "    print(\"Uri: \" + uri)\n",
    "    f = open(uri[1:] + '.html','r')\n",
    "    sub_doc = f.read()\n",
    "\n",
    "    soup_t_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "    b = soup_t_sub.findAll(\"p\")\n",
    "        \n",
    "    for i in b:\n",
    "        st = re.sub(r'\\<[^>]*\\>' ,'',str(i))\n",
    "        \n",
    "        st = re.sub(r'(\\[|\\().*?(\\]|\\))','', st)\n",
    "        translator = str.maketrans('', '', \",\")\n",
    "    \n",
    "        st = st.translate(translator)\n",
    "        st = strip_accents(st)\n",
    "        k = re.search(r\"(\\b[A-Z]+\\w*\\b)\\s+(?:\\b[a-z]*\\b\\s+)*(born|progeny|daughter|father|mother|son|offspring)(?:\\s+\\b\\w*\\b\\s+)*(?:\\s+of\\s+)(?:\\b\\w*\\b\\s)?(\\b[A-Z]+[a-z]+\\b)(?:\\s+and\\s+)?(?:\\b[a-z]*\\b\\s+)*(\\b[A-Z]+[a-z]+\\b)?\", st)\n",
    "        if k: \n",
    "            divinity_name = mapping_uri[uri].lower()\n",
    "            if divinity_name == \"tartarus\":\n",
    "                print(st)\n",
    "                print(k.group(3))\n",
    "                print(k.group(4))\n",
    "            a = str(k.group(1))\n",
    "            b = str(k.group(2))\n",
    "            c = str(k.group(3))\n",
    "            d = str(k.group(4))\n",
    "            if a.lower() == divinity_name:\n",
    "                #wiki/Pallas\n",
    "                #pallas daughter of _ and _\n",
    "                if(b in ('daughter','born','son', 'offspring','progeny')):\n",
    "                    parents_mapping.setdefault(a,[]).extend([c, d])\n",
    "                #pallas mother of _ and _\n",
    "                elif(k.group(2) in ('father','mother')):\n",
    "                    children_mapping.setdefault(a,[]).extend([c, d])\n",
    " \n",
    "            elif c.lower() == divinity_name or d.lower() == divinity_name:\n",
    "                #x son of pallas and _\n",
    "                if(k.group(2) in ('daughter','born','son', 'offspring','progeny')):\n",
    "                    children_mapping.setdefault(divinity_name.title(),[]).append(k.group(1))\n",
    "                #y father of pallas and _\n",
    "                elif(k.group(2) in ('father','mother')):\n",
    "                    parents_mapping.setdefault(divinity_name.title(),[]).append(k.group(1))\n",
    "            else:\n",
    "                incomplete_results.setdefault(uri,[]).append([index, k]) \n",
    "                print(\"Incomplete: \" + uri + \" : \" + k.group(1))\n",
    "    #print()\n",
    "    return {\"Parents\": parents_mapping, \"Children\": children_mapping, \"Incomplete\": incomplete_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(master_dict)\n",
    "master_incomplete_results = []\n",
    "def processIncompleteList(missingList, mapping_name, mapping_uri):\n",
    "    results = {}\n",
    "    incomplete_results = [] #Record those with incomplete results\n",
    "    for idx, p in enumerate(missingList):\n",
    "        print(\"Current Index: \" + str(idx))\n",
    "        results = fill_in_missing(mapping_name[p.lower()], idx, mapping_uri)\n",
    "        print(results)\n",
    "        if len(results[\"Incomplete\"]) > 0:\n",
    "            incomplete_results.append(results)\n",
    "        if(len(results[\"Parents\"]) > 0):\n",
    "            if p.lower() in master_dict and \"parents\" in master_dict[p.lower()]:\n",
    "                master_dict[p.lower()][\"parents\"] += list(results[\"Parents\"].values())[0]\n",
    "            else:\n",
    "                master_dict[p.lower()] = {\"parents\": list(results[\"Parents\"].values())[0]}\n",
    "        if(len(results[\"Children\"]) > 0):\n",
    "            if p.lower() in master_dict and \"children\" in master_dict[p.lower()]:\n",
    "                #print(master_dict[p.lower])\n",
    "                master_dict[p.lower()][\"children\"] += list(results[\"Children\"].values())[0]\n",
    "            else:\n",
    "                master_dict[p.lower()] = {\"children\": list(results[\"Children\"].values())[0]}\n",
    "        print()\n",
    "    return incomplete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prime processing\n",
    "master_incomplete_results += processIncompleteList(missing_parents, prime_mappings_name, prime_mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_incomplete_results)\n",
    "for i in master_incomplete_results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, p in enumerate(missing_children):\n",
    "#    print(\"Current Index: \" + str(idx))\n",
    "#    results = fill_in_missing(prime_mappings_name[p.lower()], idx)\n",
    "#    print(results)\n",
    "#    if len(results[\"Incomplete\"]) > 0:\n",
    "#        incomplete_results.append(results)\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Ourea\n",
    "#Tatarus should be father of Giants\n",
    "for i in master_incomplete_results:\n",
    "    print(i)\n",
    "#tatarus = master_incomplete_results[0] #Father of the Giants\n",
    "#achlys = master_incomplete_results[5] #Daughter of Nyx\n",
    "\n",
    "#master_dict['tartarus']['children'] = [\"giants\"]\n",
    "master_dict['achlys']['parents'].append('nyx')\n",
    "master_incomplete_results.clear() #discard rest of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prime_mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(major_mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prime_mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(master_dict))\n",
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = heading_content_dict[\"Titans and Titanesses\"]\n",
    "soup_titan = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_titan.find_all(\"tr\")\n",
    "#print(values[2:])\n",
    "values = values[2:]\n",
    "\n",
    "titan_set = set()\n",
    "\n",
    "titan_parents = {}\n",
    "titan_children = {}\n",
    "\n",
    "titan_uris = []\n",
    " \n",
    "titan_mappings_uri = {}  #uri to name\n",
    "titan_mappings_name = {} #name to uri\n",
    "for tr in values:\n",
    "    #print(tr.contents)\n",
    "    if len(tr.contents) < 4:\n",
    "        continue\n",
    "    if tr.contents[3].a:\n",
    "        f = open(tr.contents[3].a['href'][1:] + '.html','r')\n",
    "        titan_uris.append(tr.contents[3].a['href'][1:])\n",
    "        sub_doc = f.read()\n",
    "        soup_titan_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "        box_info = soup_titan_sub.find_all(\"tr\")\n",
    "        print(tr.contents[3].a['href'])\n",
    "        \n",
    "        vr = re.sub(r'(\\[|\\().*?(\\]|\\))','',tr.contents[3].a['href']).strip().replace('_','')\n",
    "        \n",
    "        titan_mappings_uri[tr.contents[3].a['href']] = vr[vr.rfind(\"/\") + 1:].lower()\n",
    "        titan_mappings_name[vr[vr.rfind(\"/\") + 1:].lower()] = tr.contents[3].a['href']\n",
    "        \n",
    "        titan_set.add(vr[vr.rfind(\"/\") + 1:])\n",
    "        for tr in box_info:\n",
    "            if tr.th and (tr.th.text == \"Children\" or tr.th.text == \"Parents\" or tr.th.text == \"Offspring\"):\n",
    "                st = re.sub(r'\\s(or|and)\\s', ',',tr.td.text) #Get rid of 'or' 'and' stop words\n",
    "                #st = tr.td.text\n",
    "                st = st.replace('with', '')\n",
    "                result = [x.strip().lower() for x in re.split(r',|\\n', re.sub(r'(\\[|\\().*?(\\]|\\))','',st)) if x != '']\n",
    "                #v = re.sub(r'(\\[|\\().*?(\\]|\\))','',tr.contents[3].a['href']).replace('_','').strip()\n",
    "               \n",
    "                print(tr.th.text + \":\")\n",
    "                print(result) #Parse out citation square brackets\n",
    "                \n",
    "                if tr.th.text == \"Children\" or tr.th.text == \"Offspring\" or tr.th.text == \"Parents\":\n",
    "                    #master_dict.setdefault()\n",
    "                    if vr[vr.rfind(\"/\") + 1:].lower() in master_dict:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = dict(master_dict[vr[vr.rfind(\"/\") + 1:].lower()],**{str(tr.th.text).lower(): result})\n",
    "                    else:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                        \n",
    "                    if tr.th.text == \"Children\" or tr.th.text == \"Offspring\":\n",
    "                        titan_children[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                    elif tr.th.text == \"Parents\":\n",
    "                        titan_parents[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "\n",
    "        print()\n",
    "\n",
    "missing_parents = []\n",
    "missing_children = []\n",
    "for p in titan_set:\n",
    "    if p not in titan_parents:\n",
    "        #print(\"No Parent(s): \" + p)\n",
    "        missing_parents.append(p)\n",
    "    if p not in titan_children:\n",
    "        #print(\"No Children(s):\" + p)\n",
    "        missing_children.append(p)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_parents)\n",
    "print(missing_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Titan Processing\n",
    "result = processIncompleteList(missing_parents, titan_mappings_name, titan_mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titan_mappings_name['selene']\n",
    "titan_mappings_uri['/wiki/Selene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result))\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict['tartarus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_content_dict['Gigantes and other \"giants\"']\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
