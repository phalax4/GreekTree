{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape and download all links to the local file system due to http speed limit\n",
    "import urllib.request\n",
    "html_doc = ''\n",
    "with urllib.request.urlopen('https://en.wikipedia.org/wiki/List_of_Greek_mythological_figures') as response:\n",
    "    html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "all_links = soup.find_all('a')\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all of the relevant web pages from all the links\n",
    "def download_pages():\n",
    "    counter = 0\n",
    "    for link in all_links:\n",
    "        if(link.has_attr('href') and \"/wiki/\" in link['href']):\n",
    "            if counter/len(all_links) % 10 == 0:\n",
    "                print(str(counter/len(all_links) * 100) + '%')\n",
    "            href = link['href']\n",
    "            if '.jpg' or '.svg' or '.png' in href:\n",
    "                continue\n",
    "            urllib.request.urlretrieve(\"https://en.wikipedia.org\"+href, href[href.rfind('/')+1:] + '.html')\n",
    "    href = \"/wiki/List_of_Greek_mythological_figures\"\n",
    "    urllib.request.urlretrieve(\"https://en.wikipedia.org\"+href, href[href.rfind('/')+1:] + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h3.span.text\n",
    "soup.h3.next_sibling\n",
    "sibling = soup.h3\n",
    "counter = 0\n",
    "h3_tag = soup.h3\n",
    "h3_sibling_content = soup.h3.next_sibling.next_sibling\n",
    "\n",
    "#Go back 1 sibling to be able to grab the first h3 tag into the list\n",
    "clean_siblings = [s for s in h3_tag.previous_sibling.next_siblings if (str(s).strip() != \"\" and (s.name == \"h3\" or s.name == \"div\" or s.name == \"table\" or s.name==\"ul\"))]\n",
    "clean_siblings = clean_siblings[:-7]\n",
    "clean_siblings.remove(clean_siblings[5])\n",
    "\n",
    "it = iter(clean_siblings)\n",
    "heading_content_dict= {}\n",
    "for sibling in it:\n",
    "    heading_content_dict[sibling.text.replace(\"[edit]\",\"\").replace(\"/\",\" & \")] = next(it)\n",
    "print(len(heading_content_dict))\n",
    "#print(heading_content_dict)\n",
    "print(heading_content_dict.keys())\n",
    "        \n",
    "    #print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = heading_content_dict[\"Major gods and goddesses\"]\n",
    "soup_major = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_major.find_all(\"td\")\n",
    "sub_pages = []\n",
    "\n",
    "prime_list = []\n",
    "\n",
    "major_mappings_uri = {} #uri to lowercase name\n",
    "major_mappings_name = {} #lowercase name to uri\n",
    "\n",
    "for td in values:\n",
    "    if not td.a.img:\n",
    "        print(td.a['href'])\n",
    "        f = open(td.a['href'][1:] + \".html\", 'r')\n",
    "        sub_doc = f.read()\n",
    "        soup_major_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "        box_info = soup_major_sub.find_all(\"tr\")\n",
    "        \n",
    "        vr = td.a['href']\n",
    "        major_mappings_uri[td.a['href']] = vr[vr.rfind(\"/\") + 1:].lower()\n",
    "        \n",
    "        major_mappings_name[vr[vr.rfind(\"/\") + 1:].lower()] = td.a['href']\n",
    "        \n",
    "        for tr in box_info:\n",
    "            if tr.th and (tr.th.text == \"Children\" or tr.th.text == \"Parents\" or tr.th.text == \"Offspring\"):\n",
    "                print(tr.th.text + \":\")\n",
    "                st = re.sub(r'\\s(or|and)\\s', ',',tr.td.text) #Get rid of 'or' 'and' stop words\n",
    "                #st = tr.td.text\n",
    "                result = [x.strip() for x in re.sub(r'(\\[|\\().*?(\\]|\\))','',st).split(\",\") if x != '']\n",
    "                if tr.th.text == \"Children\" or tr.th.text == \"Offspring\" or tr.th.text == \"Parents\":\n",
    "                    if vr[vr.rfind(\"/\") + 1:].lower() in master_dict:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = dict(master_dict[vr[vr.rfind(\"/\") + 1:].lower()],**{str(tr.th.text).lower(): result})\n",
    "                    else:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                        \n",
    "                    if tr.th.text == \"Children\" or tr.th.text == \"Offspring\":\n",
    "                        prime_children[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                    elif tr.th.text == \"Parents\":\n",
    "                        prime_parents[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                #master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                print(result) #Parse out citation square brackets\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = heading_content_dict[\"Primordial deities\"]\n",
    "soup_prime = BeautifulSoup(str(doc), 'html.parser')\n",
    "values = soup_prime.find_all(\"tr\")\n",
    "links = []\n",
    "\n",
    "prime_set = set()\n",
    "\n",
    "prime_parents = {}\n",
    "prime_children = {}\n",
    "\n",
    "prime_uris = []\n",
    " \n",
    "prime_mappings_uri = {}  #uri to name\n",
    "prime_mappings_name = {} #name to uri\n",
    "for tr in values:\n",
    "    if tr.contents[3].a:\n",
    "        f = open(tr.contents[3].a['href'][1:] + '.html','r')\n",
    "        prime_uris.append(tr.contents[3].a['href'][1:])\n",
    "        sub_doc = f.read()\n",
    "        soup_prime_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "        box_info = soup_prime_sub.find_all(\"tr\")\n",
    "        print(tr.contents[3].a['href'])\n",
    "        \n",
    "        vr = re.sub(r'(\\[|\\().*?(\\]|\\))','',tr.contents[3].a['href']).strip().replace('_','')\n",
    "        \n",
    "        prime_mappings_uri[tr.contents[3].a['href']] = vr[vr.rfind(\"/\") + 1:].lower()\n",
    "        prime_mappings_name[vr[vr.rfind(\"/\") + 1:].lower()] = tr.contents[3].a['href']\n",
    "        \n",
    "        prime_set.add(vr[vr.rfind(\"/\") + 1:])\n",
    "        for tr in box_info:\n",
    "            if tr.th and (tr.th.text == \"Children\" or tr.th.text == \"Parents\" or tr.th.text == \"Offspring\"):\n",
    "                st = re.sub(r'\\s(or|and)\\s', ',',tr.td.text) #Get rid of 'or' 'and' stop words\n",
    "                #st = tr.td.text\n",
    "                st = st.replace('with', '')\n",
    "                result = [x.strip() for x in re.split(r',|\\n', re.sub(r'(\\[|\\().*?(\\]|\\))','',st)) if x != ''] \n",
    "                #v = re.sub(r'(\\[|\\().*?(\\]|\\))','',tr.contents[3].a['href']).replace('_','').strip()\n",
    "               \n",
    "                print(tr.th.text + \":\")\n",
    "                print(result) #Parse out citation square brackets\n",
    "                \n",
    "                if tr.th.text == \"Children\" or tr.th.text == \"Offspring\" or tr.th.text == \"Parents\":\n",
    "                    if vr[vr.rfind(\"/\") + 1:].lower() in master_dict:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = dict(master_dict[vr[vr.rfind(\"/\") + 1:].lower()],**{str(tr.th.text).lower(): result})\n",
    "                    else:\n",
    "                        master_dict[vr[vr.rfind(\"/\") + 1:].lower()] = {str(tr.th.text).lower(): result}\n",
    "                        \n",
    "                    if tr.th.text == \"Children\" or tr.th.text == \"Offspring\":\n",
    "                        prime_children[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "                    elif tr.th.text == \"Parents\":\n",
    "                        prime_parents[vr[vr.rfind(\"/\") + 1:]] = result\n",
    "\n",
    "        print()\n",
    "\n",
    "missing_parents = []\n",
    "missing_children = []\n",
    "for p in prime_set:\n",
    "    if p not in prime_parents:\n",
    "        #print(\"No Parent(s): \" + p)\n",
    "        missing_parents.append(p)\n",
    "    if p not in prime_children:\n",
    "        #print(\"No Children(s):\" + p)\n",
    "        missing_children.append(p)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)\n",
    "#print(master_dict['aphrodite'])\n",
    "#from py2neo import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def fill_in_missing(uri, index):\n",
    "    parents_mapping = {} #current god to its parents\n",
    "    children_mapping = {} #current god to its children\n",
    "    incomplete_results = {}\n",
    "    \n",
    "    print(\"Uri: \" + uri)\n",
    "    f = open(uri[1:] + '.html','r')\n",
    "    sub_doc = f.read()\n",
    "\n",
    "    soup_t_sub = BeautifulSoup(str(sub_doc), 'html.parser')\n",
    "    b = soup_t_sub.findAll(\"p\")\n",
    "        \n",
    "    for i in b:\n",
    "        st = re.sub(r'\\<[^>]*\\>' ,'',str(i))\n",
    "        \n",
    "        st = re.sub(r'(\\[|\\().*?(\\]|\\))','', st)\n",
    "        translator = str.maketrans('', '', \",\")\n",
    "    \n",
    "        st = st.translate(translator)\n",
    "        st = strip_accents(st)\n",
    "        k = re.search(r\"(\\b[A-Z]+\\w*\\b)\\s*(?:\\b[a-z]*\\b\\s)*(progeny|daughter|father|mother|son|offspring)(\\b\\w*\\b\\s)*(?:\\s+of\\s+)(?:\\b\\w*\\b\\s)?(\\b[A-Z]+[a-z]+\\b)(?:\\sand\\s)?(\\b[A-Z]+\\w*\\b)?\", st)\n",
    "\n",
    "        #k = re.search(r\"(\\b[A-Z]+\\w*\\b)(?:\\b[a-z]*\\b\\s)*(daughter|father|mother|son|offspring)(?:\\b\\w*\\b\\s)*(?:\\s?of\\s?)(?:\\b[a-z]*\\b\\s)*(\\b[A-Z]+\\w+\\b)(?:\\sand\\s)?(\\b[A-Z]+\\w*\\b)?\", st)\n",
    "        #(daughter|father|mother|son)(?:\\s?of\\s?)(?:\\b[a-z]*\\b\\s)*(\\b[A-Z]+\\w+\\b)(?:\\sand\\s)?(\\b[A-Z]+\\w*\\b\\s)?\n",
    "        if k:\n",
    "            #print(k.group())\n",
    "            if(uri in prime_mappings_uri and str(k.group(1)).lower() == prime_mappings_uri[uri].lower()):\n",
    "                #print(prime_mappings_name[k.group(1).lower()]) \n",
    "                if(k.group(2) in ('daughter','son', 'offspring','progeny')):\n",
    "                    #Who are my parents?\n",
    "                    if k.group(1) in parents_mapping:\n",
    "                        parents_mapping[k.group(1)] = parents_mapping[k.group(1)].extend(k.group(3),k.group(4))\n",
    "                    else:\n",
    "                        parents_mapping[k.group(1)] = [k.group(3), k.group(4)]\n",
    "                elif(k.group(2) in ('father','mother')):\n",
    "                    #Who are my children?\n",
    "                    if k.group(1) in children_mapping:\n",
    "                        children_mapping[k.group(1)] = children_mapping[k.group(1)].extend(k.group(3), k.group(4))\n",
    "                    else:\n",
    "                        children_mapping[k.group(1)] = [k.group(3), k.group(4)]\n",
    "            else:\n",
    "                incomplete_results[uri] = [index, k]\n",
    "                #print(\"Incomplete: \" + uri + \" : \" + k.group(1))\n",
    "                #print(k.group())\n",
    "            print(k.group(0))\n",
    "            #print(k.group(1))\n",
    "            #print(k.group(2))\n",
    "            #print(k.group(3))\n",
    "            #print(k.group(4))\n",
    "            #print(\"++++\")            \n",
    "    #print()\n",
    "    return {\"Parents\": parents_mapping, \"Children\": children_mapping, \"Incomplete\": incomplete_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(master_dict)\n",
    "master_incomplete_results = []\n",
    "def processIncompleteList(missingList, mapping):\n",
    "    results = {}\n",
    "    incomplete_results = [] #Record those with incomplete results\n",
    "    for idx, p in enumerate(missingList):\n",
    "        print(\"Current Index: \" + str(idx))\n",
    "        results = fill_in_missing(mapping[p.lower()], idx)\n",
    "        print(results)\n",
    "        if len(results[\"Incomplete\"]) > 0:\n",
    "            incomplete_results.append(results)\n",
    "        if(len(results[\"Parents\"]) > 0):\n",
    "            if p.lower() in master_dict and \"parents\" in master_dict[p.lower()]:\n",
    "                master_dict[p.lower()][\"parents\"] += list(results[\"Parents\"].values())[0]\n",
    "            else:\n",
    "                master_dict[p.lower()] = {\"parents\": list(results[\"Parents\"].values())[0]}\n",
    "        if(len(results[\"Children\"]) > 0):\n",
    "            if p.lower() in master_dict and \"children\" in master_dict[p.lower()]:\n",
    "                #print(master_dict[p.lower])\n",
    "                master_dict[p.lower()][\"children\"] += list(results[\"Children\"].values())[0]\n",
    "            else:\n",
    "                master_dict[p.lower()] = {\"children\": list(results[\"Children\"].values())[0]}\n",
    "        print()\n",
    "    return incomplete_results\n",
    "master_incomplete_results += processIncompleteList(missing_parents, prime_mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_incomplete_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, p in enumerate(missing_children):\n",
    "#    print(\"Current Index: \" + str(idx))\n",
    "#    results = fill_in_missing(prime_mappings_name[p.lower()], idx)\n",
    "#    print(results)\n",
    "#    if len(results[\"Incomplete\"]) > 0:\n",
    "#        incomplete_results.append(results)\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_incomplete_results += processIncompleteList(missing_children, prime_mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Ourea\n",
    "#Tatarus should be father of Giants\n",
    "for i in master_incomplete_results:\n",
    "    print(i)\n",
    "tatarus = master_incomplete_results[1] #Father of the Giants\n",
    "achyls = master_incomplete_results[4] #Daughter of Nyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prime_mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(major_mappings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prime_mappings_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(master_dict))\n",
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
